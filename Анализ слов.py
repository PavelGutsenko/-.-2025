from docx import Document
import string
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords
from natasha import MorphVocab, NewsEmbedding, NewsMorphTagger, Doc as NatashaDoc, Segmenter
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# === 1. –ó–∞–≥—Ä—É–∑–∫–∞ Word-–¥–æ–∫—É–º–µ–Ω—Ç–∞ ===
doc = Document(r"c:\Users\HYPERPC\Documents\–ù–∞—É—á–Ω–∞—è —Å—Ç–∞—Ç—å—è\–ì—É—Ü–µ–Ω–∫–æ –ü–∞–≤–µ–ª. —Ä–µ—Ñ–µ—Ä–∞—Ç.docx")
text = "\n".join([para.text for para in doc.paragraphs])

# === 2. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ ===
text = text.lower()
spec_chars = string.punctuation + '¬´¬ª\t‚Äî‚Ä¶‚Äô'
text = "".join([ch for ch in text if ch not in spec_chars])
text = re.sub('\n', ' ', text)
text = "".join([ch for ch in text if ch not in string.digits])

# === 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è NLTK ===
nltk.download('punkt')
nltk.download('stopwords')
tokens = word_tokenize(text)

# === 4. –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ ===
stop_words = set(stopwords.words('russian'))

custom_exclude = {
    '–≤–æ–ø—Ä–æ—Å', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–∞–∫–∂–µ', '–æ–¥–Ω–∞–∫–æ', '–ø–æ—ç—Ç–æ–º—É', '–≤–æ–æ–±—â–µ', '–Ω–∞–ø—Ä–∏–º–µ—Ä',
    '–∫–æ–Ω–µ—á–Ω–æ', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ', '–≤—Ä–æ–¥–µ', '–≤–ø—Ä–æ—á–µ–º', '–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ', '–≤–µ—Ä–æ—è—Ç–Ω–æ',
    '–∏—Ç–∞–∫', '–ø—Ä–∏—á–µ–º', '—Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ', '–º–µ–∂–¥—É', '–ø—Ä–æ—Å—Ç–æ', '–¥–∞', '–Ω–µ—Ç', '–ª–∏', '–∂–µ',
    '–±—ã', '–±', '–ø—É—Å—Ç—å', '–≤–µ–¥—å', '—Ö–æ—Ç—è', '–µ—Å–ª–∏', '—á—Ç–æ–±—ã', '–∏', '–∞', '–Ω–æ', '–∏–ª–∏',
    '—á—Ç–æ', '–∫–∞–∫', '–∫–æ–≥–¥–∞', '–≥–¥–µ', '–∫—É–¥–∞', '–æ—Ç–∫—É–¥–∞', '—Å', '—Å–æ', '–≤', '–≤–æ', '–Ω–∞', '–Ω–∞–¥', '–ø–æ–¥',
    '–∫', '–∫–æ', '–ø–æ', '—É', '–æ', '–æ–±', '–ø—Ä–æ', '–¥–ª—è', '–∏–∑', '–∑–∞', '–ø—Ä–∏',
    '—è', '—Ç—ã', '–æ–Ω', '–æ–Ω–∞', '–æ–Ω–æ', '–º—ã', '–≤—ã', '–æ–Ω–∏', '–º–æ–π', '—Ç–≤–æ–π', '–µ–≥–æ', '–µ–µ', '–Ω–∞—à', '–≤–∞—à', '–∏—Ö',
    '–º–µ–Ω—è', '—Ç–µ–±—è', '–Ω–µ–≥–æ', '–Ω–µ–µ', '–Ω–∞—Å', '–≤–∞—Å', '–Ω–∏—Ö', '–º–Ω–µ', '—Ç–µ–±–µ', '–µ–º—É', '–µ–π', '–Ω–∞–º', '–≤–∞–º', '–∏–º',
    '–º–Ω–æ–π', '—Ç–æ–±–æ–π', '–Ω–∏–º', '–Ω–µ–π', '–Ω–∞–º–∏', '–≤–∞–º–∏', '–∏–º–∏', '–æ–±–æ–∏—Ö', '–Ω–∞–±–∏—É–ª–ª–∏–Ω', '–∫–∞—Å–∞—Ç—å—Å—è', '–≤–∏–¥–µ—Ç—å', '–∫–ª—é—á–µ–≤–æ–π', '–¥–µ–Ω–µ–∂–Ω–æ–∫—Ä–µ–¥–∏—Ç–Ω—ã–π',
    '–≥–æ–≤–æ—Ä–∏—Ç—å', '—Å—á–∏—Ç–∞—Ç—å', '—Å–∫–∞–∑–∞—Ç—å', '–∫–æ—Ç–æ—Ä—ã–π',
}

tokens_no_stopwords = [
    word for word in tokens
    if word not in stop_words
    and word not in custom_exclude
    and word.isalpha()
]

# === 5. –õ–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –∏ POS Natasha ===
morph_vocab = MorphVocab()
emb = NewsEmbedding()
morph_tagger = NewsMorphTagger(emb)
segmenter = Segmenter()

natasha_doc = NatashaDoc(" ".join(tokens_no_stopwords))
natasha_doc.segment(segmenter)
natasha_doc.tag_morph(morph_tagger)

for token in natasha_doc.tokens:
    token.lemmatize(morph_vocab)

# === 6. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —á–∞—Å—Ç–æ—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ ===
lemmas = [
    token.lemma for token in natasha_doc.tokens
    if token.text.isalpha()
    and token.lemma not in custom_exclude
]

fdist = FreqDist(lemmas)
print("\nüîù –¢–æ–ø-10 —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ª–æ–≤:")
for word, freq in fdist.most_common(10):
    print((word, freq))

count_zestkiy = sum(1 for token in natasha_doc.tokens if token.lemma == "–∂–µ—Å—Ç–∫–∏–π")
print("\n–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—è–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤–∞ '–∂–µ—Å—Ç–∫–∏–π' (–ª–µ–º–º–∞):", count_zestkiy)

# === 7. –í—ã–¥–µ–ª–µ–Ω–∏–µ –≥–ª–∞–≥–æ–ª–æ–≤ –∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö ===
verbs = [
    token.lemma for token in natasha_doc.tokens
    if token.pos in {'VERB', 'INFN'}
    and token.lemma not in custom_exclude
]

adjectives = [
    token.lemma for token in natasha_doc.tokens
    if token.pos in {'ADJ', 'ADJF', 'ADJS'}
    and token.lemma not in custom_exclude
]

verb_freq = FreqDist(verbs)
adj_freq = FreqDist(adjectives)

print("\nüîß –¢–æ–ø-10 –≥–ª–∞–≥–æ–ª–æ–≤:")
for word, freq in verb_freq.most_common(10):
    print((word, freq))

print("\nüé® –¢–æ–ø-10 –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö:")
for word, freq in adj_freq.most_common(10):
    print((word, freq))

print("–ù–∞–π–¥–µ–Ω–æ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö:", len(adjectives))
print(adjectives[:10])

# === 8. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–±–ª–∞–∫–æ–≤ —Å–ª–æ–≤ ===
font_path = "C:/Windows/Fonts/arial.ttf"  # ‚ö†Ô∏è —É–∫–∞–∂–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π —à—Ä–∏—Ñ—Ç —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π

wordcloud_all = WordCloud(
    width=1000, height=700,
    background_color="white",
    colormap="viridis",
    font_path=font_path,
    max_words=100
).generate_from_frequencies(fdist)

wordcloud_verbs = WordCloud(
    width=1000, height=700,
    background_color="white",
    colormap="plasma",
    font_path=font_path,
    max_words=100
).generate_from_frequencies(verb_freq)

wordcloud_adjs = WordCloud(
    width=1000, height=700,
    background_color="white",
    colormap="inferno",
    font_path=font_path,
    max_words=100
).generate_from_frequencies(adj_freq)

# === 9. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
plt.figure(figsize=(18, 12))

plt.subplot(1, 3, 1)
plt.imshow(wordcloud_all, interpolation="bilinear")
plt.axis("off")
plt.title("–í—Å–µ —Å–ª–æ–≤–∞", fontsize=16)

plt.subplot(1, 3, 2)
plt.imshow(wordcloud_verbs, interpolation="bilinear")
plt.axis("off")
plt.title("–ì–ª–∞–≥–æ–ª—ã", fontsize=16)

plt.subplot(1, 3, 3)
plt.imshow(wordcloud_adjs, interpolation="bilinear")
plt.axis("off")
plt.title("–ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ", fontsize=16)

plt.tight_layout()
plt.show()
